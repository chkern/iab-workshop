---
title: "Random Forests and Boosting"
author: "Christoph Kern"
date: "19 March 2018"
output:
  html_document: default
  html_notebook: default
---

## Setup

```{r results='hide', message=FALSE, warning=FALSE}
library(foreach)
library(caret)
library(rpart)
library(randomForest)
library(xgboost)
library(pdp)
library(PRROC)
```

## Data

Here we use data from the UCI Machine Learning repository on drug consumption. The data contains records for 1885 respondents with personality measurements (e.g. Big-5), level of education, age, gender, country of residence and ethnicity as features. In addition, information on the usage of 18 drugs is included. 

Source: https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29

```{r}
drugs <- read.csv("drug_consumption.data", header = FALSE)
names(drugs) <- c("ID", "Age", "Gender", "Education", "Country", "Ethnicity", "Neuroticism", "Extraversion", "Openness", "Agreeableness", "Conscientiousness", "Impulsive", "SS", "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis", "Choc", "Coke", "Crack", "Ecstasy", "Heroin", "Ketamine", "Legalh", "LSD", "Meth", "Mushrooms", "Nicotine", "Semer", "VSA")
drugs$Age <- as.factor(drugs$Age)
drugs$Gender  <- as.factor(drugs$Gender)
drugs$Education  <- as.factor(drugs$Education)
drugs$Country <- as.factor(drugs$Country)
drugs$Ethnicity <- as.factor(drugs$Ethnicity)
head(drugs)
```

For this notebook, we build a dummy variable on LSD usage as our outcome of interest.

```{r}
drugs$D_LSD <- "LSD"
drugs$D_LSD[drugs$LSD == "CL0"] <- "no LSD"
drugs$D_LSD <- as.factor(drugs$D_LSD)
drugs$D_LSD <- relevel(drugs$D_LSD, "no LSD")
table(drugs$LSD, drugs$D_LSD)
summary(drugs$D_LSD)
```

Then we split the data into a training and a test part. This time we use `createDataPartition` from `caret`, which samples within the levels of the outcome variable when splitting the data.

```{r}
set.seed(456)
inTrain <- createDataPartition(drugs$D_LSD, 
                               p = .8, 
                               list = FALSE, 
                               times = 1)

drugs_train <- drugs[inTrain,]
drugs_test <- drugs[-inTrain,]
```

## ML models

### Random Forest

In this notebook we use the `caret` package for building prediction models, which offers a lot of useful functions. For this, we first specify our evaluation method. In the following, we use 5-Fold Cross-Validation.

```{r}
ctrl  <- trainControl(method = "cv",
                      number = 5)
```

Next, a grid object can be used to specify a set of try-out values for tuning. For random forest, we primarily have to care about `mtry`, i.e. the number of features to sample at each split point.

```{r}
grid <- expand.grid(mtry = c(2,4,6))
```

Both objects can be passed on to `train`, along with the specification of the model and the method. For Random Forest, we use `rf`.

```{r}
set.seed(744)
rf <- train(D_LSD ~ Age + Gender + Education + Neuroticism + Extraversion +
             Openness + Agreeableness + Conscientiousness + Impulsive + SS,
            data = drugs_train,
            method = "rf",
            trControl = ctrl,
            tuneGrid = grid,
            metric = "Kappa",
            importance = TRUE)
```

Now calling the random forest object lists the results of the tuning process.

```{r}
rf
```

Plotting the final model gives us an idea of how the error evolves as more trees are added.

```{r, fig.align="center"}
plot(rf$finalModel)
```

With random forests, the individual trees of the ensemble typically look quite different. To get an idea of the components of the forest, `getTree` can be used to list individual trees.

```{r}
getTree(rf$finalModel, k = 1, labelVar = T)[1:10,]
getTree(rf$finalModel, k = 2, labelVar = T)[1:10,]
```

Calculating variable importance helps us interpreting results from ensemble methods since now we have grown a lot of trees.

```{r}
varImp(rf)
```

Especially for ensemble methods, plots can be useful in order to see how the features are related to the outcome according to the fitted model. This can be done separately by predictor... 

```{r, fig.align="center"}
pdp1 <- partial(rf, pred.var = "Openness", trim.outliers = T)
pdp2 <- partial(rf, pred.var = "Impulsive", trim.outliers = T)
p1 <- plotPartial(pdp1, rug = T, train = drugs_train, alpha = 0.3)
p2 <- plotPartial(pdp2, rug = T, train = drugs_train, alpha = 0.3)
grid.arrange(p1, p2, ncol = 2)
```

...and also by considering multiple predictors jointly.

```{r, fig.align="center"}
pdp3 <- partial(rf, pred.var = c("Openness", "Impulsive"))
plotPartial(pdp3, levelplot = F, drape = T, colorkey = F, screen = list(z = -45, x = -60))
```

### Boosting

For Gradient Boosting, we have to take care of a couple of tuning parameters. Here, we use `xgboost` and build a grid with all combinations of a set of try-out values. See `?xgboost` for information on the available tuning parameters.

```{r}
grid <- expand.grid(max_depth = c(1, 2, 3),
                    nrounds = c(500, 750, 1000),
                    eta = c(0.05, 0.01),
                    min_child_weight = 10,
                    subsample = 0.7,
                    gamma = 0,
                    colsample_bytree = 1)
grid
```
 
Again, this is passed on to `train`, now using `xgbTree` instead of `rf`.

```{r}
set.seed(744)
xgb <- train(D_LSD ~ Age + Gender + Education + Neuroticism + Extraversion +
             Openness + Agreeableness + Conscientiousness + Impulsive + SS,
             data = drugs_train,
             method = "xgbTree",
             trControl = ctrl,
             tuneGrid = grid,
             metric = "Kappa")
```

Instead of just printing the results from the tuning process, we can also plot them.

```{r, fig.align="center"}
plot(xgb)
```

### CART

Adding a single tree for comparison...

```{r}
set.seed(744)
cart <- train(D_LSD ~ Age + Gender + Education + Neuroticism + Extraversion +
             Openness + Agreeableness + Conscientiousness + Impulsive + SS,
              data = drugs_train,
              method = "rpart2",
              trControl = ctrl,
              metric = "Kappa")
cart
```

### Logistic regression

...and also a logistic regression model.

```{r}
set.seed(744)
logit <- train(D_LSD ~ Age + Gender + Education + Neuroticism + Extraversion +
             Openness + Agreeableness + Conscientiousness + Impulsive + SS,
             data = drugs_train,
             method = "glm",
             trControl = ctrl)
```

We may want to take a glimpse at the regression results.

```{r}
summary(logit)
```

## Comparison

After we ran a bunch of models, we can use `resamples` to gather the cross-validation results from all of them.

```{r}
resamps <- resamples(list(RandomForest = rf,
                          Boosting = xgb,
                          CART = cart,
                          Logit = logit))
```

This object can now be used for comparing these models with respect to their performance, based on CV in the training set.

```{r, fig.align="center"}
bwplot(resamps)
```

## Prediction

For evaluating performance, we predict the outcome in the test set in two formats. We can use `predict` for predicting class membership (here for all methods) and also for computing predicted probabilities (here for the xgboost result).

```{r}
p_rf1 <- predict(rf, newdata = drugs_test)
p_xgb1 <- predict(xgb, newdata = drugs_test)
p_cart1 <- predict(cart, newdata = drugs_test)
p_logit1 <- predict(logit, newdata = drugs_test)

p_xgb2 <- predict(xgb, newdata = drugs_test, type = "prob")
```

Given predicted class membership, we can use the function `confusionMatrix` for evaluating our classification models.

```{r}
confusionMatrix(p_rf1, drugs_test$D_LSD)
confusionMatrix(p_xgb1, drugs_test$D_LSD)
confusionMatrix(p_cart1, drugs_test$D_LSD)
confusionMatrix(p_logit1, drugs_test$D_LSD)
```

Additionally, ROC curves are helpful for evaluating prediction performance with categorical outcomes. Here we could (e.g.) use the `PRROC` package, which has a function called `roc.curve`.

```{r}
fg <- p_xgb2[drugs_test$D_LSD == "LSD", 2]
bg <- p_xgb2[drugs_test$D_LSD == "no LSD", 2]
roc <- roc.curve(scores.class0 = fg, scores.class1 = bg, curve = T)
```

Finally, we can print and plot the resulting roc object.

```{r, fig.align="center"}
roc
plot(roc)
```

## Excursus: Bagging with GLMs

At this point, it might be interesting to see whether GLMs also benefit from Bagging. To investigate this, we build our own Bagging ensemble with a foreach loop. In this loop, we first sample $n$ observations from our training data with replacement. Then, a logistic regression model is estimated using this sample and the respective test set predictions from this model are stored in the object `y_rbag` (Note that there are other ways to bag logistic regression).

```{r}
y_rbag <- foreach(m = 1:100, .combine = cbind) %do% { 
  rows <- sample(nrow(drugs_train), replace = T)
  fit <- glm(D_LSD ~ Age + Gender + Education + Neuroticism + Extraversion +
             Openness + Agreeableness + Conscientiousness + Impulsive + SS,
             family = binomial,
             data = drugs_train[rows,])
  predict(fit, newdata = drugs_test, type = "response")
}
```

Now we predict class membership based on the predicted probabilities.

```{r}
y_rbagc <- as.data.frame(y_rbag)
y_rbagc[y_rbag > 0.5] <- "LSD"
y_rbagc[y_rbag <= 0.5] <- "no LSD"
```

On this basis, we add a column to our prediction data that represents the majority class vote over all predicted classes. 

```{r}
y_rbagc$vote <- apply(y_rbagc, 1, function(x) {
  xtab <- table(unlist(x))
  maxclass <- names(xtab)[xtab %in% max(xtab)]
  sample(maxclass, size = 1)
})
```

Finally, we can compare the performance of a single logistic regression model from our Bagging loop with the performance of the ensemble vote. Since logistic regression can be considered to be a low variance method, we do not see huge differences.

```{r}
confusionMatrix(y_rbagc[,1], drugs_test$D_LSD, mode = "prec_recall")
confusionMatrix(y_rbagc$vote, drugs_test$D_LSD, mode = "prec_recall")
```

## References

* E. Fehrman, A. K. Muhammad, E. M. Mirkes, V. Egan and A. N. Gorban (2015). The Five Factor Model of personality and evaluation of drug consumption risk. https://arxiv.org/abs/1506.06297.

