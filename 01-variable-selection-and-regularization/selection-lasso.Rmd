---
title: "Variable Selection and Regularization"
author: "Christoph Kern"
date: "19 March 2018"
output:
  html_document: default
  html_notebook: default
---

## Setup

```{r results='hide', message=FALSE, warning=FALSE}
library(mlbench)
library(ggplot2)
library(GGally)
library(glmnet)
```

## Data

In this notebook, we use the Boston Housing data set. "This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms."

Source: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html

```{r}
data(BostonHousing2)
head(BostonHousing2)
names(BostonHousing2)
```

Since we want to compare the performance of some regularized models at the end of the modeling process, we first split the data into a training and a test part. This can be done by random sampling with `sample`.

```{r}
set.seed(7345)
train <- sample(1:nrow(BostonHousing2), 0.8*nrow(BostonHousing2))
boston_train <- BostonHousing2[train,]
boston_test <- BostonHousing2[-train,]
```

A quick look on our outcome variable for the next sections, which is the Median value of owner-occupied homes in $1000's.

```{r}
summary(boston_train$medv)
summary(boston_test$medv)
```

Another look at the outcome variable, now via boxplots separated by the Charles River dummy variable (1 if tract bounds river; 0 otherwise).

```{r}
p1 <- qplot(chas, medv, data=boston_train, geom=c("boxplot"), fill=chas)
p1 + theme(axis.ticks = element_blank(), axis.text.x = element_blank())
```

Some data exploration using `ggpairs` from the `GGally` package...

```{r, results="hide", fig.align="center"}
ggpairs(boston_train[,c(5,7,14,19)], lower = list(continuous = "smooth"))
```

## Regularized regression

Now we can prepare our training data for the regularized regression models. The `glmnet` package needs models to be fitted on an X matrix and an y vector, which we need to generate first.

```{r}
X <- model.matrix(medv ~ . - town - tract - cmedv,
                  boston_train)[,-1]
y <- boston_train$medv
```

### Ridge regression

To estimate a sequence of regularized models we pass our X and y objects to the `glmnet` function. Setting alpha to zero equals to fitting ridge regression models. By default, `glmnet` figures out an appropriate  series of lambda values.

```{r}
m1 <- glmnet(X, y, alpha = 0)
summary(m1)
```

Let's see how we can access the results from glmnet...

```{r}
m1$lambda[1]
m1$lambda[100]
m1$beta[,1]
m1$beta[,ncol(m1$beta)]
```

A nice feature of `glmnet` is that we can easily plot the coefficient paths by simply calling `plot` in connection with our results object.

```{r}
plot(m1, label=T)
plot(m1, label=T, xvar = "lambda")
```

However, at this point we do not know which lambda leads to the best model. Defining "best" in terms of prediction performance for new data, Cross-Validation can be used for this task. 

```{r}
m1_cv <- cv.glmnet(X, y, alpha = 0)
plot(m1_cv)
```

On this basis, we can now have a look at the models that perform best in terms of the smallest CV error and with respect to the 1-SE rule. We also store the value of lambda that corresponds to the smallest CV error for later usage.

```{r}
coef(m1_cv, s = "lambda.min")
coef(m1_cv, s = "lambda.1se")
bestlam1 <- m1_cv$lambda.min
bestlam1
```

### Lasso

To estimate a Lasso sequence, we simply call `glmnet` again and set alpha to one. 

```{r}
m2 <- glmnet(X, y, alpha = 1)
```

Here we display the first, last and one in-between model of our model series. We see that coefficients are eventually shrunken exactly to zero as the penalty on model complexity increases.

```{r}
m2$lambda[1]
m2$lambda[(ncol(m2$beta)/2)]
m2$lambda[ncol(m2$beta)]
m2$beta[,1]
m2$beta[,(ncol(m2$beta)/2)]
m2$beta[,ncol(m2$beta)]
```

This also becomes clear when plotting the coefficient paths.

```{r}
plot(m2, label=T, xvar = "lambda")
```

When using Cross-Validation with Lasso, we see that a full model with all features leads not necessarily to the best model in terms of prediction performance.

```{r}
m2_cv <- cv.glmnet(X, y, alpha = 1)
plot(m2_cv)
```

Again, we may have a look at the model with the smallest CV error and store the corresponding lambda.

```{r}
coef(m2_cv, s = "lambda.min")
bestlam2 <- m2_cv$lambda.min
```

### Elastic net

In addition to ridge regression and the lasso, the elastic net can be used as a compromise between the former approaches. Here we build a small tuning loop that estimates series of regularized models for three settings of the mixing parameter `alpha`. 

```{r}
a <- c(0.1, 0.5, 0.9)
m3_cv <- foreach(i = a, .combine = rbind) %do% {
  cv <- cv.glmnet(X, y, alpha = i)
  data.frame(cvm = cv$cvm, lambda = cv$lambda, lambda.min = cv$lambda.min, alpha = i)
}
head(m3_cv)
```

Based on the former CV loop we select the lambda and alpha constellation that is associated with the smallest CV error.

```{r}
b3_cv <- m3_cv[m3_cv$cvm == min(m3_cv$cvm),]
m3 <- glmnet(X, y, lambda =  b3_cv$lambda, alpha = b3_cv$alpha)
coef(m3)
```

### Prediction in test set

Finally, we investigate the performance of our models in the test set. For this task, we construct an X matrix from the test set.

```{r}
Xt <- model.matrix(medv ~ . - town - tract - cmedv,
                  boston_test)[,-1]
```

This matrix can be used in the `predict` function, along with the respective model that should be used for prediction. We try out our best ridge, lasso and elastic net model. One can also add a "null model" with a huge penalty for comparison purposes.

```{r}
p_ridge <- predict(m1, s = bestlam1, newx = Xt)
p_lasso <- predict(m2, s = bestlam2, newx = Xt)
p_net <- predict(m3, newx = Xt)

p_null <- predict(m2, s = 1e10, newx = Xt)
```

As a last step, let's look at the test MSE of our models.

```{r}
mean((p_null - boston_test$medv)^2)

mean((p_ridge - boston_test$medv)^2)
mean((p_lasso - boston_test$medv)^2)
mean((p_net - boston_test$medv)^2)
```

## References

* https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
